{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2721468959588019\n",
      "0.39682963220260037\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "#                              help='path to junctions for querying '\n",
    "import os \n",
    "#user specified inputs needed for precision & recall calculations (prc)\n",
    "sample_ids = ['18538','28168', '45103', '46189', '7613'] \n",
    "#motif to filter junctions by\n",
    "filter_motif = 'GTAG'\n",
    "#scaled coverage threshold tot filter junctions by \n",
    "cov_thresh = 0\n",
    "sample_num = '28168'\n",
    "#names of columns in the file that is read in as a data frame \n",
    "dataframe_col_names = ['chromosome', 'start', 'end', 'length', 'strand', \n",
    "                       '5prime_motif', '3prime_motif', 'sampleIDs:coverages', \n",
    "                       'num_samples', 'total_coverage']\n",
    "#names of columns that specify junction coordinates & sample ID; used for prc \n",
    "col_names_for_prc = ['chromosome', 'start', 'end', 'strand', sample_num]\n",
    "\n",
    "def find_coverages(sample_ids,cov_col_name, dataframe):\n",
    "    #processes input data frame, calculates scaled coverage values \n",
    "    #for each sample, adds column with sample id and scaled coverages\n",
    "    #for each junction to the input data frame \n",
    "    cov_strings = list(dataframe[cov_col_name]) \n",
    "    cov_values = []\n",
    "    samples = dict.fromkeys(sample_ids)\n",
    "    cum_cov = 0   #sum of total coverage values across samples\n",
    "    for sample_id in sample_ids: \n",
    "        cov_values = []\n",
    "        sample_id_string = ',' + sample_id + ':'                    \n",
    "        for cov_string in cov_strings: \n",
    "            #print(sample_string)\\n',\n",
    "            if sample_id_string in cov_string: \n",
    "                #isolates sample of interest from other samples found in junction\n",
    "                first_split = cov_string.split(sample_id_string) \n",
    "                #isolates cov value for sample of interest\n",
    "                second_split = first_split[1].split(',')\n",
    "                cov_value = int(second_split[0]) \n",
    "            else: \n",
    "                cov_value = 0 \n",
    "            cov_values.append(cov_value)\n",
    "        cum_cov = cum_cov + sum(cov_values)\n",
    "        samples[sample_id] = (cov_values)\n",
    "    #value used to scale coverages across samples \n",
    "    avg_total_cov = cum_cov/(len(sample_ids))\n",
    "    #adds scaled coverages to dataframe to a new column for the sample \n",
    "    for sample_id in sample_ids: \n",
    "        dataframe[sample_id] = [cov_value/avg_total_cov for cov_value in samples[sample_id]]\n",
    "        \n",
    "#reading ground truth data (srav2 data set) from file into data frame\n",
    "srav2_data = pd.read_csv('srav2.junctions.test_set.tsv',   \n",
    "                  sep='\\t',\n",
    "                  header=None, \n",
    "                  names=dataframe_col_names)\n",
    "#adding coverages for samples to ground truth data frame\n",
    "find_coverages(sample_ids, 'sampleIDs:coverages', srav2_data)\n",
    "\n",
    "#reading test data (srav2 data set) from file into data drame\n",
    "srav3_data = pd.read_csv('srav3.junctions.test_set.tsv',   #reading test data\n",
    "                  sep='\\t',\n",
    "                  header=None, \n",
    "                  names=dataframe_col_names)\n",
    "\n",
    "#adding coverages for samples to test data frame\n",
    "find_coverages(sample_ids, 'sampleIDs:coverages', srav3_data) \n",
    "\n",
    "\n",
    "#adding column with the full motif for each junction to dataframes\n",
    "srav2_data['motif'] = srav2_data['5prime_motif']  + srav2_data['3prime_motif'] \n",
    "srav3_data['motif'] = srav3_data['5prime_motif']  + srav3_data['3prime_motif']\n",
    "\n",
    "#creating new data frames for ground truth & test that are filtered by user\n",
    "#specified thresholds for the motif and scaled coverage values \n",
    "srav2_data_filtered = srav2_data[(srav2_data['motif'] == filter_motif) \n",
    "                                & (srav2_data[sample_num] >= cov_thresh)]\n",
    "\n",
    "srav3_data_filtered = srav3_data[(srav3_data['motif'] == filter_motif) \n",
    "                                 & (srav3_data[sample_num] >= cov_thresh)]\n",
    "\n",
    "\n",
    "#calculating precision and recall \n",
    "\n",
    "#number of samples in ground truth\n",
    "srav2_num_juncs = srav2_data_filtered.shape[0]\n",
    "#number of samples in test \n",
    "srav3_num_juncs = srav3_data_filtered.shape[0] \n",
    "\n",
    "merged_data = pd.merge(srav2_data_filtered[col_names_for_prc], \n",
    "                       srav3_data_filtered[col_names_for_prc], \n",
    "                       how = 'inner')\n",
    "\n",
    "#number of samples shared by ground truth and test \n",
    "num_shared_juncs = merged_data.shape[0]\n",
    "\n",
    "precision = num_shared_juncs/srav3_num_juncs\n",
    "recall = num_shared_juncs/srav2_num_juncs\n",
    "\n",
    "\n",
    "print(precision)\n",
    "print(recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
